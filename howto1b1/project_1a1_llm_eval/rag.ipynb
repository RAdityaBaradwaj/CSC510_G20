{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6adc0e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Downloaded: 2023-02484.pdf\n",
      "✓ Downloaded: FoodCodeRuleRevision-SignificantAnalysis.pdf\n",
      "✗ Skipped: https://templates.upmetrics.co/wp-content/uploads/2022/07/food-delivery-business-plan-example.pdf -> HTTPSConnectionPool(host='templates.upmetrics.co', port=443): Max retries exceeded with url: /wp-content/uploads/2022/07/food-delivery-business-plan-example.pdf (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)')))\n",
      "✗ Skipped: https://jungleworks.com/download/Guide-UberEats.pdf -> 403 Client Error: Forbidden for url: https://jungleworks.com/download/Guide-UberEats.pdf\n",
      "✓ Downloaded: JFDR55.2_4_Peng.pdf\n",
      "✓ Downloaded: 882.pdf\n",
      "✗ Skipped: https://www.emerald.com/bpmj/article-pdf/29/8/210/1735135/bpmj-04-2023-0308.pdf -> 403 Client Error: Forbidden for url: https://www.emerald.com/bpmj/article-pdf/29/8/210/1735135/bpmj-04-2023-0308.pdf\n",
      "✓ Downloaded: id611.pdf\n",
      "✗ Skipped: https://www.emerald.com/ijchm/article-pdf/33/4/1129/763798/ijchm-05-2020-0477.pdf -> 403 Client Error: Forbidden for url: https://www.emerald.com/ijchm/article-pdf/33/4/1129/763798/ijchm-05-2020-0477.pdf\n",
      "✓ Downloaded: Vol.13%20No.5.37.pdf\n",
      "✓ Downloaded: paper.pdf\n",
      "✗ Skipped: https://irjaes.com/wp-content/uploads/2023/05/IRJAES-V8N2P205Y23.pdf -> 406 Client Error: Not Acceptable for url: https://irjaes.com/wp-content/uploads/2023/05/IRJAES-V8N2P205Y23.pdf\n",
      "✓ Downloaded: id601.pdf\n",
      "✓ Downloaded: ba6c61f24ca22c12ba2d400f8e6ba711c010.pdf\n",
      "✓ Downloaded: JHTC_Vol7Issue1_Khan_case.pdf\n",
      "✓ Downloaded: 644.pdf\n",
      "✓ Downloaded: IJRPR28409.pdf\n",
      "✓ Downloaded: 324.pdf\n",
      "✗ Skipped: https://www.emerald.com/bfj/article-pdf/125/13/164/196542/bfj-08-2022-0694.pdf -> 403 Client Error: Forbidden for url: https://www.emerald.com/bfj/article-pdf/125/13/164/196542/bfj-08-2022-0694.pdf\n",
      "✓ Downloaded: 11782-case-studies-food-loss-and-waste-in-north-america-en.pdf\n",
      "✗ Skipped: https://www.researchpublish.com/upload/book/Customer%20Satisfaction-14022022-2.pdf -> 406 Client Error: Not Acceptable for url: https://www.researchpublish.com/upload/book/Customer%20Satisfaction-14022022-2.pdf\n",
      "✓ Downloaded: Inevitable%2C%20vulnerable%2C%20unprofitable-an%20inquiry%20into%20food%20delivery%20platforms%20in%20Europe_2024.pdf\n",
      "✓ Downloaded: &path_info=OFD_Platforms_Final_Submission.pdf\n",
      "✓ Downloaded: 861943-1255479.pdf\n",
      "✓ Downloaded: 1201-1602245215.pdf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, requests, os\n",
    "\n",
    "# export your Google Sheet as CSV first\n",
    "SPREADSHEET_CSV = \"data/supplementals/supplementals.csv\"\n",
    "PDF_DIR = \"docs\"\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(SPREADSHEET_CSV)\n",
    "links = df.stack().dropna().astype(str).tolist()\n",
    "\n",
    "for url in links:\n",
    "    if url.lower().endswith(\".pdf\"):\n",
    "        try:\n",
    "            fname = url.split(\"/\")[-1] or \"download.pdf\"\n",
    "            path = os.path.join(PDF_DIR, fname)\n",
    "            r = requests.get(url, timeout=20, verify=True)\n",
    "            r.raise_for_status()\n",
    "            with open(path, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            print(\"✓ Downloaded:\", fname)\n",
    "        except Exception as e:\n",
    "            print(\"✗ Skipped:\", url, \"->\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c6a70c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp313-cp313-macosx_14_0_arm64.whl.metadata (5.1 kB)\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.105.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.11.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.8.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.7.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.1-cp313-cp313-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Downloading pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/adityab/miniconda3/envs/se25/lib/python3.13/site-packages (from sentence-transformers) (4.15.0)\n",
      "Collecting filelock (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/adityab/miniconda3/envs/se25/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/adityab/miniconda3/envs/se25/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2025.9.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Users/adityab/miniconda3/envs/se25/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/adityab/miniconda3/envs/se25/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/adityab/miniconda3/envs/se25/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/adityab/miniconda3/envs/se25/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/adityab/miniconda3/envs/se25/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: setuptools in /Users/adityab/miniconda3/envs/se25/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (78.1.1)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading transformers-4.56.0-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.9-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.0-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.12.0-cp313-cp313-macosx_14_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Downloading openai-1.105.0-py3-none-any.whl (928 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m928.2/928.2 kB\u001b[0m \u001b[31m350.7 kB/s\u001b[0m  \u001b[33m0:00:02\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.10.0-cp313-cp313-macosx_11_0_arm64.whl (318 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.11.0-cp313-cp313-macosx_11_0_arm64.whl (997 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.1/997.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl (171 kB)\n",
      "Downloading regex-2025.9.1-cp313-cp313-macosx_11_0_arm64.whl (286 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading torch-2.8.0-cp313-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m  \u001b[33m0:00:21\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.1-cp313-cp313-macosx_12_0_arm64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.16.1-cp313-cp313-macosx_14_0_arm64.whl (20.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: mpmath, typing-inspection, tqdm, threadpoolctl, sympy, sniffio, scipy, safetensors, regex, pyyaml, PyPDF2, pydantic-core, Pillow, networkx, MarkupSafe, joblib, jiter, hf-xet, h11, fsspec, filelock, faiss-cpu, distro, annotated-types, tiktoken, scikit-learn, pydantic, jinja2, huggingface-hub, httpcore, anyio, torch, tokenizers, httpx, transformers, openai, sentence-transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37/37\u001b[0m [sentence-transformers]ence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 Pillow-11.3.0 PyPDF2-3.0.1 annotated-types-0.7.0 anyio-4.10.0 distro-1.9.0 faiss-cpu-1.12.0 filelock-3.19.1 fsspec-2025.9.0 h11-0.16.0 hf-xet-1.1.9 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.34.4 jinja2-3.1.6 jiter-0.10.0 joblib-1.5.2 mpmath-1.3.0 networkx-3.5 openai-1.105.0 pydantic-2.11.7 pydantic-core-2.33.2 pyyaml-6.0.2 regex-2025.9.1 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.16.1 sentence-transformers-5.1.0 sniffio-1.3.1 sympy-1.14.0 threadpoolctl-3.6.0 tiktoken-0.11.0 tokenizers-0.22.0 torch-2.8.0 tqdm-4.67.1 transformers-4.56.0 typing-inspection-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityab/miniconda3/envs/se25/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# rag.ipynb\n",
    "\n",
    "# === 0. Setup ===\n",
    "%pip install sentence-transformers faiss-cpu PyPDF2 openai tiktoken\n",
    "\n",
    "import os, re, hashlib, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import faiss\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tiktoken\n",
    "\n",
    "PDF_DIR = \"docs\"        # put your PDFs here\n",
    "PAGE_FILE = \"page.file\"\n",
    "TOPK = 6\n",
    "CHUNK_TOKENS = 150      # target tokens per chunk\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7f860a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Helpers ===\n",
    "\n",
    "def file_sig(path: Path):\n",
    "    h = hashlib.md5()\n",
    "    h.update(str(path.stat().st_mtime_ns).encode())\n",
    "    h.update(str(path.stat().st_size).encode())\n",
    "    return h.hexdigest()\n",
    "\n",
    "def load_texts(pdf_path: Path):\n",
    "    reader = PdfReader(str(pdf_path))\n",
    "    out = []\n",
    "    for i, page in enumerate(reader.pages, start=1):\n",
    "        text = page.extract_text() or \"\"\n",
    "        if text.strip():\n",
    "            out.append((text, {\"doc\": pdf_path.name, \"page\": i}))\n",
    "    return out\n",
    "\n",
    "def chunk_text_tokens(text, meta, max_tokens=CHUNK_TOKENS):\n",
    "    words = text.split()\n",
    "    chunks, metas, cur, cur_len = [], [], [], 0\n",
    "    for w in words:\n",
    "        tlen = len(tokenizer.encode(w))\n",
    "        if cur_len + tlen > max_tokens:\n",
    "            chunks.append(\" \".join(cur))\n",
    "            metas.append({**meta, \"chunk\": len(chunks)})\n",
    "            cur, cur_len = [], 0\n",
    "        cur.append(w); cur_len += tlen\n",
    "    if cur:\n",
    "        chunks.append(\" \".join(cur))\n",
    "        metas.append({**meta, \"chunk\": len(chunks)})\n",
    "    return chunks, metas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2020d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Build pagefile ===\n",
    "\n",
    "def build_pagefile(pdf_dir=PDF_DIR, path=PAGE_FILE):\n",
    "    all_chunks, all_metas = [], []\n",
    "    for pdf in Path(pdf_dir).glob(\"*.pdf\"):\n",
    "        for t, meta in load_texts(pdf):\n",
    "            cs, ms = chunk_text_tokens(t, meta)\n",
    "            all_chunks.extend(cs); all_metas.extend(ms)\n",
    "    X = np.asarray(model.encode(all_chunks, convert_to_numpy=True), \"float32\")\n",
    "    ix = faiss.IndexFlatL2(X.shape[1]); ix.add(X)\n",
    "    manifest = {str(p): file_sig(p) for p in Path(pdf_dir).glob(\"*.pdf\")}\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump({\"ix\": ix, \"X\": X, \"chunks\": all_chunks,\n",
    "                     \"metas\": all_metas, \"manifest\": manifest}, f)\n",
    "    return ix, all_chunks, all_metas\n",
    "\n",
    "def load_pagefile(path=PAGE_FILE):\n",
    "    with open(path, \"rb\") as f:\n",
    "        pf = pickle.load(f)\n",
    "    return pf[\"ix\"], pf[\"chunks\"], pf[\"metas\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28370cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Query ===\n",
    "\n",
    "def query_rag(query, ix, chunks, metas, k=TOPK):\n",
    "    qvec = np.asarray(model.encode([query], convert_to_numpy=True), \"float32\")\n",
    "    D, I = ix.search(qvec, k)\n",
    "    results = []\n",
    "    for rank, (d, idx) in enumerate(zip(D[0], I[0]), 1):\n",
    "        m = metas[idx]; snip = chunks[idx][:120].replace(\"\\n\",\" \")\n",
    "        results.append((rank, d, m[\"doc\"], m[\"page\"], snip))\n",
    "    return results\n",
    "\n",
    "def show_results(results):\n",
    "    print(\"# Semantic Page Table\")\n",
    "    for r in results:\n",
    "        print(f\"{r[0]:>2}. L2^2={r[1]:.3f}  {r[2]} p{r[3]}  '{r[4]}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ade00a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. Export Bullet Notes ===\n",
    "\n",
    "def export_notes(results, outfile=\"bullet_notes.md\"):\n",
    "    with open(outfile, \"w\") as f:\n",
    "        f.write(\"# Bullet Notes (Auto-Generated)\\n\\n\")\n",
    "        for rank, d, doc, page, snip in results:\n",
    "            f.write(f\"- **{doc} p{page}**: {snip.strip()} …\\n\")\n",
    "    print(f\"Saved {outfile}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54c745dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Semantic Page Table\n",
      " 1. L2^2=1.129  FoodCodeRuleRevision-SignificantAnalysis.pdf p42  'food. Benefits: The proposed change has the benefit of potentially reducing risk of foodborne illness for populations th'\n",
      " 2. L2^2=1.242  11782-case-studies-food-loss-and-waste-in-north-america-en.pdf p50  'Kingdom. OECD Foo d, Agriculture and Fisheries Papers 76. Pingree, C. 2016. Introducing commonsense bill to standardize '\n",
      " 3. L2^2=1.265  FoodCodeRuleRevision-SignificantAnalysis.pdf p4  '(Title, Number or Code Year Only); (3) Incorporate legislation passed by state legislature; (4) Incorpora te RCW or rule'\n",
      " 4. L2^2=1.304  FoodCodeRuleRevision-SignificantAnalysis.pdf p42  'specific cost estimates . One respondent indicated they repackage a lo t of food and will have to arrange for another vo'\n",
      " 5. L2^2=1.315  2023-02484.pdf p12  'burden of the proposed collection of information, including the validity of the methodology and assumptions used; (c) wa'\n",
      " 6. L2^2=1.316  JFDR55.2_4_Peng.pdf p17  'of importance have been identified, technical requirements (i.e., the service design requirements ) need to be identifie'\n",
      "Saved data/supplementals/notes.md\n"
     ]
    }
   ],
   "source": [
    "# === Run RAG on all PDFs ===\n",
    "\n",
    "# 1. Build (or rebuild) the index from everything in docs/\n",
    "ix, chunks, metas = build_pagefile(PDF_DIR, PAGE_FILE)\n",
    "\n",
    "# 2. Ask a query\n",
    "query = \"What are the allergen labeling requirements?\"\n",
    "results = query_rag(query, ix, chunks, metas, k=6)\n",
    "\n",
    "# 3. Show results\n",
    "show_results(results)\n",
    "\n",
    "# 4. Export notes (optional, to supplementals/)\n",
    "export_notes(results, outfile=\"data/supplementals/notes.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc20fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_all_pdfs(pdf_dir=PDF_DIR):\n",
    "    for pdf in Path(pdf_dir).glob(\"*.pdf\"):\n",
    "        base = pdf.stem\n",
    "        results = query_rag(f\"Summarize the key compliance and tax rules from {base}\", ix, chunks, metas, k=8)\n",
    "        outfile = f\"data/supplementals/{base}_notes.md\"\n",
    "        export_notes(results, outfile)\n",
    "        print(\"✓ Saved notes for\", base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5d9d9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/supplementals/11782-case-studies-food-loss-and-waste-in-north-america-en_notes.md\n",
      "✓ Saved notes for 11782-case-studies-food-loss-and-waste-in-north-america-en\n",
      "Saved data/supplementals/Vol.13%20No.5.37_notes.md\n",
      "✓ Saved notes for Vol.13%20No.5.37\n",
      "Saved data/supplementals/paper_notes.md\n",
      "✓ Saved notes for paper\n",
      "Saved data/supplementals/861943-1255479_notes.md\n",
      "✓ Saved notes for 861943-1255479\n",
      "Saved data/supplementals/IJRPR28409_notes.md\n",
      "✓ Saved notes for IJRPR28409\n",
      "Saved data/supplementals/882_notes.md\n",
      "✓ Saved notes for 882\n",
      "Saved data/supplementals/JHTC_Vol7Issue1_Khan_case_notes.md\n",
      "✓ Saved notes for JHTC_Vol7Issue1_Khan_case\n",
      "Saved data/supplementals/2023-02484_notes.md\n",
      "✓ Saved notes for 2023-02484\n",
      "Saved data/supplementals/644_notes.md\n",
      "✓ Saved notes for 644\n",
      "Saved data/supplementals/ba6c61f24ca22c12ba2d400f8e6ba711c010_notes.md\n",
      "✓ Saved notes for ba6c61f24ca22c12ba2d400f8e6ba711c010\n",
      "Saved data/supplementals/1201-1602245215_notes.md\n",
      "✓ Saved notes for 1201-1602245215\n",
      "Saved data/supplementals/324_notes.md\n",
      "✓ Saved notes for 324\n",
      "Saved data/supplementals/JFDR55.2_4_Peng_notes.md\n",
      "✓ Saved notes for JFDR55.2_4_Peng\n",
      "Saved data/supplementals/id601_notes.md\n",
      "✓ Saved notes for id601\n",
      "Saved data/supplementals/FoodCodeRuleRevision-SignificantAnalysis_notes.md\n",
      "✓ Saved notes for FoodCodeRuleRevision-SignificantAnalysis\n",
      "Saved data/supplementals/id611_notes.md\n",
      "✓ Saved notes for id611\n",
      "Saved data/supplementals/&path_info=OFD_Platforms_Final_Submission_notes.md\n",
      "✓ Saved notes for &path_info=OFD_Platforms_Final_Submission\n",
      "Saved data/supplementals/Inevitable%2C%20vulnerable%2C%20unprofitable-an%20inquiry%20into%20food%20delivery%20platforms%20in%20Europe_2024_notes.md\n",
      "✓ Saved notes for Inevitable%2C%20vulnerable%2C%20unprofitable-an%20inquiry%20into%20food%20delivery%20platforms%20in%20Europe_2024\n"
     ]
    }
   ],
   "source": [
    "summarize_all_pdfs(PDF_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6a72c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def export_all_notes(pdf_dir=PDF_DIR, outfile=\"supplementals/all_notes.md\"):\n",
    "    Path(\"supplementals\").mkdir(exist_ok=True, parents=True)\n",
    "    with open(outfile, \"w\") as f:\n",
    "        f.write(\"# Combined Bullet Notes from All PDFs\\n\\n\")\n",
    "        for pdf in Path(pdf_dir).glob(\"*.pdf\"):\n",
    "            base = pdf.stem\n",
    "            # query the index for each doc\n",
    "            results = query_rag(f\"Summarize the key compliance and tax rules from {base}\", ix, chunks, metas, k=8)\n",
    "            f.write(f\"## {base}\\n\\n\")\n",
    "            for rank, d, doc, page, snip in results:\n",
    "                f.write(f\"- **{doc} p{page}**: {snip.strip()} …\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    print(\"✓ Saved\", outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59641783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved supplementals/all_notes.md\n"
     ]
    }
   ],
   "source": [
    "# Build/reload the index (if not already built)\n",
    "ix, chunks, metas = build_pagefile(PDF_DIR, PAGE_FILE)\n",
    "\n",
    "# Export combined notes\n",
    "export_all_notes(PDF_DIR, outfile=\"supplementals/all_notes.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda5534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
